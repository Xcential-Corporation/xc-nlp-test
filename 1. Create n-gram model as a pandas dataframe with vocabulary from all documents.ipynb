{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document and Section Similarity Score Predictor using Tfidf Vectorizer with Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import re, string\n",
    "import os\n",
    "from os import path, listdir\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join\n",
    "from types import new_class\n",
    "from typing import List\n",
    "from lxml import etree \n",
    "from contextlib import ExitStack\n",
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer, TreebankWordTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Among the larger bills is samples/congress/116/BILLS-116s1790enr.xml (~ 10MB)\n",
    "\n",
    "PATH_116_USLM = 'data/samples/congress/116/uslm'\n",
    "PATH_117_USLM = 'data/samples/congress/117/uslm'\n",
    "PATH_116_USLM_TRAIN = 'data/samples/congress/116/train'\n",
    "PATH_116_TEXT = 'data/samples/congress/116/txt'\n",
    "\n",
    "BILLS_SAMPLE = [f'BILLS-116hr{number}ih.xml' for number in range(100, 300)]\n",
    "BIG_BILLS = ['BILLS-116s1790enr.xml', 'BILLS-116hjres31enr.xml']\n",
    "BIG_BILLS_PATHS = [path.join(PATH_116_USLM, bill) for bill in (BIG_BILLS + BILLS_SAMPLE)]\n",
    "\n",
    "SAMPLE_BILL_PATHS_TRAIN = [join(PATH_116_USLM_TRAIN, f) for f in listdir(PATH_116_USLM) if isfile(join(PATH_116_USLM_TRAIN, f))]\n",
    "SAMPLE_BILL_PATHS = [join(PATH_117_USLM, f) for f in listdir(PATH_117_USLM) if isfile(join(PATH_117_USLM, f))]\n",
    "\n",
    "\n",
    "NAMESPACES = {'uslm': 'http://xml.house.gov/schemas/uslm/1.0'}\n",
    "\n",
    "\n",
    "def get_filepaths(dirpath: str, reMatch = r'.xml$') -> List[str]:\n",
    "    return [join(dirpath, f) for f in listdir(dirpath) if (len(re.findall(reMatch, f)) > 0) and isfile(join(dirpath, f))]\n",
    "\n",
    "def getEnum(section) -> str:\n",
    "  enumpath = section.xpath('enum')  \n",
    "  if len(enumpath) > 0:\n",
    "    return enumpath[0].text\n",
    "  return ''\n",
    "\n",
    "def getHeader(section) -> str:\n",
    "  headerpath = section.xpath('header')  \n",
    "  if len(headerpath) > 0:\n",
    "    return headerpath[0].text\n",
    "  return ''\n",
    "\n",
    "def text_to_vect(txt: str , ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets ngrams from text\n",
    "    \"\"\"\n",
    "    # See https://stackoverflow.com/a/32128803/628748\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(txt)\n",
    "    #vect = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "    #    tokenizer=TreebankWordTokenizer().tokenize, lowercase=True)\n",
    "    vect = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "        tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    vect.fit(sentences)\n",
    "    # ngrams = vect.get_feature_names_out()\n",
    "    # print('{1}-grams: {0}'.format(ngrams, ngram_size))\n",
    "    #print(vect.vocabulary_)\n",
    "    return vect # list of text documents\n",
    "\n",
    "def xml_to_sections(xml_path: str):\n",
    "    \"\"\"\n",
    "    Parses the xml file into sections \n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    sections = billTree.xpath('//uslm:section', namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return []\n",
    "    return [{\n",
    "            'section_number': getEnum(section) ,\n",
    "            'section_header':  getHeader(section),\n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } if (section.xpath('header') and len(section.xpath('header')) > 0  and section.xpath('enum') and len(section.xpath('enum'))>0) else\n",
    "        {\n",
    "            'section_number': '',\n",
    "            'section_header': '', \n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } \n",
    "        for section in sections ]\n",
    "\n",
    "def xml_to_text(xml_path: str, level: str = 'section', separator: str = '\\n*****\\n') -> str:\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    #return etree.tostring(billTree, method=\"text\", encoding=\"unicode\")\n",
    "    # Use 'body' for level to get the whole body element\n",
    "    sections = billTree.xpath('//uslm:'+level, namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return '' \n",
    "    return separator.join([etree.tostring(section, method=\"text\", encoding=\"unicode\") for section in sections])\n",
    "\n",
    "def xml_to_vect(xml_paths: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    total_str = '\\n'.join([xml_to_text(xml_path) for xml_path in xml_paths])\n",
    "    return text_to_vect(total_str, ngram_size=ngram_size)\n",
    "\n",
    "    # to get the vocab dict: vect.vocabulary_\n",
    "\n",
    "def combine_vocabs(vocabs: List[TfidfVectorizer]):\n",
    "    \"\"\"\n",
    "    Combines one or more vocabs into one\n",
    "    \"\"\"\n",
    "    vocab_keys = list(set([list(v.vocabulary_.keys()) for v in vocabs]))\n",
    "    vocab = {vocab_key: str(i) for i, vocab_key in enumerate(vocab_keys)}\n",
    "    return vocab\n",
    "\n",
    "def get_combined_vocabs(xml_paths: List[str] = SAMPLE_BILL_PATHS, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets the combined vocabulary of all the xml files\n",
    "    \"\"\"\n",
    "    return xml_to_vect(xml_paths, ngram_size=ngram_size)\n",
    "\n",
    "def getSampleText(level = 'body'):\n",
    "    return xml_to_text(BIG_BILLS_PATHS[0])\n",
    "\n",
    "def transform_text(text: str, vocab: dict, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Transforms text into a vector using the vocab\n",
    "    \"\"\"\n",
    "    return TfidfVectorizer(vocabulary=vocab).fit_transform([text])\n",
    "\n",
    "def train_Tfidf_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a Tfidf vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X \n",
    "\n",
    "\n",
    "def xml_samples_to_text(dirpath: str, level: str = 'section', separator: str = '\\n*****\\n'):\n",
    "    \"\"\"\n",
    "    Converts xml files in a directory to txt files\n",
    "    \"\"\"\n",
    "    xfiles = get_filepaths(dirpath)\n",
    "    for xfile in xfiles:\n",
    "        with open(xfile.replace('.xml', f'-{level}s.txt'), 'w') as f:\n",
    "            f.write(xml_to_text(xfile, level=level, separator=separator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text \n",
    "def text_cleaning(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create document and sections data list after xml document level and section level parsing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_document_and_sections_data_list(directory_path):\n",
    "    \n",
    "    #create data lists \n",
    "    doc_corpus_data=[]\n",
    "    section_corpus_data = []\n",
    "\n",
    "    #get all xml files from data directory for parsing\n",
    "    bill_files = [f for f in os.listdir(directory_path) if f.endswith('.xml')]\n",
    "\n",
    "    print(len(bill_files))\n",
    "    bill_files = bill_files[:10]\n",
    "\n",
    "    #iterate over all bill files\n",
    "    for i in range(0, len(bill_files)):\n",
    "\n",
    "        #indexing bill document file\n",
    "        bill_doc_file = bill_files[i]\n",
    "\n",
    "        #parse xml into sections\n",
    "        secs = xml_to_sections(os.path.join(directory_path, bill_doc_file))\n",
    "\n",
    "        #check  of sections should be 1 or more than 1\n",
    "        if(len(secs)>0):  \n",
    "\n",
    "            #intialize string variable for document content\n",
    "            doc_content = \"\"\n",
    "\n",
    "            #iterate over all parse sections text of bill doc file\n",
    "            for s_number, section in enumerate(secs):  \n",
    "\n",
    "                #text cleaning applied on each section text\n",
    "                sec_text = text_cleaning(section['section_text'])\n",
    "\n",
    "                #concatenate section text to doc content \n",
    "                doc_content = doc_content + sec_text + \" \"\n",
    "\n",
    "                 #for now sentence id is sentence number in document\n",
    "                section_corpus_data.append([Path(bill_doc_file).stem[:], s_number, sec_text ])\n",
    "\n",
    "            doc_corpus_data.append([Path(bill_doc_file).stem[:], doc_content])\n",
    "\n",
    "\n",
    "    return doc_corpus_data, section_corpus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create pandas_dataframe for storing bill document and sections data with their ID and Text\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_pandas_dataframe_for_bill_document_and_sections(doc_corpus_data, section_corpus_data):\n",
    "    \n",
    "    \n",
    "    # doc_corpus_columns\n",
    "    doc_corpus_columns = [\"Bill ID\", 'Document Text']\n",
    "    doc_corpus_data_df = pd.DataFrame(doc_corpus_data, columns = doc_corpus_columns)\n",
    "    \n",
    "    #section_corpus_columns\n",
    "    section_corpus_columns = [\"Bill ID\", 'Section Number', 'Section Text']\n",
    "    section_corpus_data_df = pd.DataFrame(section_corpus_data, columns = section_corpus_columns)\n",
    "    \n",
    "    return doc_corpus_data_df, section_corpus_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save Vocabulary Corpus of Bill Documents and Section to Parquet File\n",
    "\"\"\"\n",
    "\n",
    "def save_bill_documents_and_section_corpuses_to_parquet_file(doc_corpus_data_df, section_corpus_data_df):\n",
    "\n",
    "    doc_corpus_data_df.to_parquet('doc_corpus_data.parquet', engine='fastparquet')\n",
    "    section_corpus_data_df.to_parquet('section_corpus_data.parquet', engine='fastparquet')\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Load Vocabulary Corpus of Bill Documents and Section From Parquet File\n",
    "\"\"\"\n",
    "def load_bill_documents_and_section_corpuses_from_parquet_file():\n",
    "\n",
    "    doc_corpus_data_df = pd.read_parquet('doc_corpus_data.parquet', engine='fastparquet')\n",
    "    section_corpus_data_df = pd.read_parquet('section_corpus_data.parquet', engine='fastparquet')\n",
    "    \n",
    "    return doc_corpus_data_df, section_corpus_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train TfIDF Vectorizer for Bill Documents and Sections using Pandas Dataframe\n",
    "\"\"\"\n",
    "\n",
    "def train_tfidf_vectorizer_on_bill_document_and_sections(doc_corpus_data_df, section_corpus_data_df):\n",
    "\n",
    "    # Vectorizer to convert a collection of raw documents to a matrix \n",
    "    doc_tfidf_vectorizer = TfidfVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    #Fit tfidf vectorize instance on document level corpus\n",
    "    tv_doc_matrix = doc_tfidf_vectorizer.fit_transform(doc_corpus_data_df['Document Text'])\n",
    "\n",
    "    # Vectorizer to convert a collection of sections to a matrix \n",
    "    sec_tfidf_vectorizer = TfidfVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    #Fit tfidf vectorize instance on section level corpus\n",
    "    tv_section_matrix = sec_tfidf_vectorizer.fit_transform(section_corpus_data_df['Section Text'])\n",
    "    \n",
    "    return doc_tfidf_vectorizer, sec_tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Store TfIDF Vectorizer Model for Bill Documents and Sections\n",
    "\"\"\"\n",
    "\n",
    "def store_bill_document_and_section_tfidf_vectorizer(doc_tfidf_vectorizer, sec_tfidf_vectorizer):\n",
    "    \n",
    "    # save tfidf vectorize instance for doc_tfidf_vectorizer\n",
    "    pickle.dump(doc_tfidf_vectorizer, open(\"doc_tfidf_vectorizer.pickel\", \"wb\"))\n",
    "\n",
    "    #save tfidf vectorize instance for sec_tfidf_vectorizer\n",
    "    pickle.dump(sec_tfidf_vectorizer, open(\"sec_tfidf_vectorizer.pickel\", \"wb\"))\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Load TfIDF Vectorizer Model for Bill Documents and Sections\n",
    "\"\"\"    \n",
    "def laod_bill_document_and_section_tfidf_vectorizer():\n",
    "    \n",
    "    # load tfidf vectorize instance for doc_tfidf_vectorizer\n",
    "    doc_tfidf_vectorizer = pickle.load(open(\"doc_tfidf_vectorizer.pickel\", \"rb\"))\n",
    "\n",
    "\n",
    "    # load tfidf vectorize instance for sec_tfidf_vectorizer\n",
    "    sec_tfidf_vectorizer = pickle.load(open(\"sec_tfidf_vectorizer.pickel\", \"rb\"))\n",
    "    \n",
    "    return doc_tfidf_vectorizer, sec_tfidf_vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9168\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    #create document and sections data list after xml document level and section level parsing\n",
    "    \n",
    "    directory_path = PATH_117_USLM\n",
    "    doc_corpus_data, section_corpus_data = create_document_and_sections_data_list(directory_path)\n",
    "    \n",
    "    \n",
    "    #create pandas_dataframe for storing bill document and sections data with their ID and Text\n",
    "    doc_corpus_data_df, section_corpus_data_df = create_pandas_dataframe_for_bill_document_and_sections(doc_corpus_data, section_corpus_data)\n",
    "    \n",
    "    \n",
    "    #Save Vocabulary Corpus of Bill Documents and Section to Parquet File\n",
    "    save_bill_documents_and_section_corpuses_to_parquet_file(doc_corpus_data_df, section_corpus_data_df)\n",
    "    \n",
    "    #Train TfIDF Vectorizer for Bill Documents and Sections using Pandas Dataframe\n",
    "    doc_tfidf_vectorizer, sec_tfidf_vectorizer = train_tfidf_vectorizer_on_bill_document_and_sections(doc_corpus_data_df, section_corpus_data_df)\n",
    "    store_bill_document_and_section_tfidf_vectorizer(doc_tfidf_vectorizer, sec_tfidf_vectorizer)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
