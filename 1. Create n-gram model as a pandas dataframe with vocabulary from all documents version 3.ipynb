{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document and Section Similarity Score Predictor using Count Vectorizer with Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import re, string\n",
    "import os\n",
    "from os import path, listdir\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join\n",
    "from types import new_class\n",
    "from typing import List\n",
    "from lxml import etree \n",
    "from contextlib import ExitStack\n",
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer, TreebankWordTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Among the larger bills is samples/congress/116/BILLS-116s1790enr.xml (~ 10MB)\n",
    "\n",
    "PATH_116_USLM = 'data/samples/congress/116/uslm'\n",
    "PATH_117_USLM = 'data/samples/congress/117/uslm'\n",
    "PATH_116_USLM_TRAIN = 'data/samples/congress/116/train'\n",
    "PATH_116_TEXT = 'data/samples/congress/116/txt'\n",
    "\n",
    "BILLS_SAMPLE = [f'BILLS-116hr{number}ih.xml' for number in range(100, 300)]\n",
    "BIG_BILLS = ['BILLS-116s1790enr.xml', 'BILLS-116hjres31enr.xml']\n",
    "BIG_BILLS_PATHS = [path.join(PATH_116_USLM, bill) for bill in (BIG_BILLS + BILLS_SAMPLE)]\n",
    "\n",
    "SAMPLE_BILL_PATHS_TRAIN = [join(PATH_116_USLM_TRAIN, f) for f in listdir(PATH_116_USLM) if isfile(join(PATH_116_USLM_TRAIN, f))]\n",
    "SAMPLE_BILL_PATHS = [join(PATH_117_USLM, f) for f in listdir(PATH_117_USLM) if isfile(join(PATH_117_USLM, f))]\n",
    "\n",
    "\n",
    "NAMESPACES = {'uslm': 'http://xml.house.gov/schemas/uslm/1.0'}\n",
    "\n",
    "\n",
    "def get_filepaths(dirpath: str, reMatch = r'.xml$') -> List[str]:\n",
    "    return [join(dirpath, f) for f in listdir(dirpath) if (len(re.findall(reMatch, f)) > 0) and isfile(join(dirpath, f))]\n",
    "\n",
    "def getEnum(section) -> str:\n",
    "  enumpath = section.xpath('enum')  \n",
    "  if len(enumpath) > 0:\n",
    "    return enumpath[0].text\n",
    "  return ''\n",
    "\n",
    "def getHeader(section) -> str:\n",
    "  headerpath = section.xpath('header')  \n",
    "  if len(headerpath) > 0:\n",
    "    return headerpath[0].text\n",
    "  return ''\n",
    "\n",
    "def text_to_vect(txt: str , ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets ngrams from text\n",
    "    \"\"\"\n",
    "    # See https://stackoverflow.com/a/32128803/628748\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(txt)\n",
    "    #vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "    #    tokenizer=TreebankWordTokenizer().tokenize, lowercase=True)\n",
    "    vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "        tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    vect.fit(sentences)\n",
    "    # ngrams = vect.get_feature_names_out()\n",
    "    # print('{1}-grams: {0}'.format(ngrams, ngram_size))\n",
    "    #print(vect.vocabulary_)\n",
    "    #print(\"list of text documents: \", vect)\n",
    "    return vect # list of text documents\n",
    "\n",
    "def xml_to_sections(xml_path: str):\n",
    "    #print(xml_path)\n",
    "    \n",
    "    \"\"\"\n",
    "    Parses the xml file into sections \n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    sections = billTree.xpath('//uslm:section', namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return []\n",
    "    return [{\n",
    "            'section_number': getEnum(section) ,\n",
    "            'section_header':  getHeader(section),\n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } if (section.xpath('header') and len(section.xpath('header')) > 0  and section.xpath('enum') and len(section.xpath('enum'))>0) else\n",
    "        {\n",
    "            'section_number': '',\n",
    "            'section_header': '', \n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } \n",
    "        for section in sections ]\n",
    "\n",
    "def xml_to_text(xml_path: str, level: str = 'section', separator: str = '\\n*****\\n') -> str:\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    #return etree.tostring(billTree, method=\"text\", encoding=\"unicode\")\n",
    "    # Use 'body' for level to get the whole body element\n",
    "    sections = billTree.xpath('//uslm:'+level, namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return '' \n",
    "    return separator.join([etree.tostring(section, method=\"text\", encoding=\"unicode\") for section in sections])\n",
    "\n",
    "def xml_to_vect(xml_paths: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    total_str = '\\n'.join([xml_to_text(xml_path) for xml_path in xml_paths])\n",
    "    return text_to_vect(total_str, ngram_size=ngram_size)\n",
    "\n",
    "    # to get the vocab dict: vect.vocabulary_\n",
    "\n",
    "def combine_vocabs(vocabs: List[CountVectorizer]):\n",
    "    \"\"\"\n",
    "    Combines one or more vocabs into one\n",
    "    \"\"\"\n",
    "    vocab_keys = list(set([list(v.vocabulary_.keys()) for v in vocabs]))\n",
    "    vocab = {vocab_key: str(i) for i, vocab_key in enumerate(vocab_keys)}\n",
    "    return vocab\n",
    "\n",
    "def get_combined_vocabs(xml_paths: List[str] = SAMPLE_BILL_PATHS, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets the combined vocabulary of all the xml files\n",
    "    \"\"\"\n",
    "    return xml_to_vect(xml_paths, ngram_size=ngram_size)\n",
    "\n",
    "def getSampleText(level = 'body'):\n",
    "    return xml_to_text(BIG_BILLS_PATHS[0])\n",
    "\n",
    "def transform_text(text: str, vocab: dict, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Transforms text into a vector using the vocab\n",
    "    \"\"\"\n",
    "    return CountVectorizer(vocabulary=vocab).fit_transform([text])\n",
    "\n",
    "def train_Count_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a Count vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X \n",
    "\n",
    "\n",
    "def xml_samples_to_text(dirpath: str, level: str = 'section', separator: str = '\\n*****\\n'):\n",
    "    \"\"\"\n",
    "    Converts xml files in a directory to txt files\n",
    "    \"\"\"\n",
    "    xfiles = get_filepaths(dirpath)\n",
    "    for xfile in xfiles:\n",
    "        with open(xfile.replace('.xml', f'-{level}s.txt'), 'w') as f:\n",
    "            f.write(xml_to_text(xfile, level=level, separator=separator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text \n",
    "def text_cleaning(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create document and sections data list after xml document level and section level parsing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_document_and_sections_data_list(directory_path):\n",
    "    \n",
    "    #create data lists \n",
    "    doc_corpus_data=[]\n",
    "    section_corpus_data = []\n",
    "\n",
    "    #get all xml files from data directory for parsing\n",
    "    bill_files = [f for f in os.listdir(directory_path) if f.endswith('.xml')]\n",
    "\n",
    "    print(len(bill_files))\n",
    "    bill_files = bill_files[:10]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #iterate over all bill files\n",
    "    for i in range(0, len(bill_files)):\n",
    "\n",
    "        #indexing bill document file\n",
    "        bill_doc_file = bill_files[i]\n",
    "\n",
    "        #parse xml into sections\n",
    "        secs = xml_to_sections(os.path.join(directory_path, bill_doc_file))\n",
    "        \n",
    "        \n",
    "\n",
    "        #check  of sections should be 1 or more than 1\n",
    "        if(len(secs)>0):  \n",
    "\n",
    "            #intialize string variable for document content\n",
    "            doc_content = \"\"\n",
    "\n",
    "            #iterate over all parse sections text of bill doc file\n",
    "            for s_number, section in enumerate(secs):  \n",
    "\n",
    "                #text cleaning applied on each section text\n",
    "                sec_text = text_cleaning(section['section_text'])\n",
    "                \n",
    "                #print(text_to_vect(sec_text, ngram_size=4))\n",
    "\n",
    "                #concatenate section text to doc content \n",
    "                doc_content = doc_content + sec_text + \" \"\n",
    "\n",
    "                 #for now sentence id is sentence number in document\n",
    "                section_corpus_data.append([Path(bill_doc_file).stem[:], s_number, sec_text ])\n",
    "\n",
    "            doc_corpus_data.append([Path(bill_doc_file).stem[:], doc_content])\n",
    "\n",
    "\n",
    "    return doc_corpus_data, section_corpus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create pandas_dataframe for storing bill document and sections data with their ID and Text\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_pandas_dataframe_for_bill_document_and_sections(doc_corpus_data, section_corpus_data):\n",
    "    \n",
    "    \n",
    "    # doc_corpus_columns\n",
    "    doc_corpus_columns = [\"Bill ID\", 'Document Text']\n",
    "    doc_corpus_data_df = pd.DataFrame(doc_corpus_data, columns = doc_corpus_columns)\n",
    "    \n",
    "    #section_corpus_columns\n",
    "    section_corpus_columns = [\"Bill ID\", 'Section Number', 'Section Text']\n",
    "    section_corpus_data_df = pd.DataFrame(section_corpus_data, columns = section_corpus_columns)\n",
    "    \n",
    "    return doc_corpus_data_df, section_corpus_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save documents dataframe of Bill Documents and Section to Parquet File\n",
    "\"\"\"\n",
    "\n",
    "def save_bill_documents_and_section_corpuses_to_parquet_file(doc_corpus_data_df, section_corpus_data_df):\n",
    "\n",
    "    doc_corpus_data_df.to_parquet('doc_corpus_data_df.parquet', engine='fastparquet')\n",
    "    section_corpus_data_df.to_parquet('section_corpus_data_df.parquet', engine='fastparquet')\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Load documents dataframe of Bill Documents and Section From Parquet File\n",
    "\"\"\"\n",
    "def load_bill_documents_and_section_corpuses_from_parquet_file():\n",
    "\n",
    "    doc_corpus_data_df = pd.read_parquet('doc_corpus_data_df.parquet', engine='fastparquet')\n",
    "    section_corpus_data_df = pd.read_parquet('section_corpus_data_df.parquet', engine='fastparquet')\n",
    "    \n",
    "    return doc_corpus_data_df, section_corpus_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save Vocabulary of Bill Documents and Section to Parquet File\n",
    "\"\"\"\n",
    "\n",
    "def save_bill_documents_and_section_vocab_to_parquet_file(doc_vocab, section_vocab):\n",
    "    \n",
    "    doc_vocab = pd.DataFrame([doc_vocab])\n",
    "    section_vocab = pd.DataFrame([section_vocab])\n",
    "\n",
    "    doc_vocab.to_parquet('doc_vocab.parquet', engine='fastparquet')\n",
    "    section_vocab.to_parquet('section_vocab.parquet', engine='fastparquet')\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Load Vocabulary of Bill Documents and Section From Parquet File\n",
    "\"\"\"\n",
    "def load_bill_documents_and_section_vocab_from_parquet_file():\n",
    "\n",
    "    doc_vocab = pd.read_parquet('doc_vocab.parquet', engine='fastparquet')\n",
    "    section_vocab = pd.read_parquet('section_vocab.parquet', engine='fastparquet')\n",
    "    \n",
    "    return doc_vocab, section_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Count Vectorizer for Bill Documents and Sections using Pandas Dataframe\n",
    "\"\"\"\n",
    "\n",
    "def train_count_vectorizer_on_bill_document_and_sections(doc_corpus_data_df, section_corpus_data_df):\n",
    "\n",
    "    # Vectorizer to convert a collection of raw documents to a matrix \n",
    "    doc_count_vectorizer = CountVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    #Fit count vectorize instance on document level corpus\n",
    "    cv_doc_matrix = doc_count_vectorizer.fit_transform(doc_corpus_data_df['Document Text'])\n",
    "    \n",
    "    \n",
    "    # Vectorizer to convert a collection of sections to a matrix \n",
    "    sec_count_vectorizer = CountVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    #Fit count vectorize instance on section level corpus\n",
    "    cv_section_matrix = sec_count_vectorizer.fit_transform(section_corpus_data_df['Section Text'])\n",
    "    \n",
    "    #print(\"CV section: \", cv_section_matrix.todense().tolist())\n",
    "    \n",
    "    #save vocabulary\n",
    "    save_bill_documents_and_section_vocab_to_parquet_file(doc_count_vectorizer.vocabulary_, sec_count_vectorizer.vocabulary_)\n",
    "  \n",
    "    return doc_count_vectorizer, sec_count_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Store Count Vectorizer Model for Bill Documents and Sections\n",
    "\"\"\"\n",
    "\n",
    "def store_bill_document_and_section_count_vectorizer(doc_count_vectorizer, sec_count_vectorizer):\n",
    "    \n",
    "    # save count vectorize instance for doc_count_vectorizer\n",
    "    pickle.dump(doc_count_vectorizer, open(\"doc_count_vectorizer.pickel\", \"wb\"))\n",
    "\n",
    "    #save count vectorize instance for sec_count_vectorizer\n",
    "    pickle.dump(sec_count_vectorizer, open(\"sec_count_vectorizer.pickel\", \"wb\"))\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Load Count Vectorizer Model for Bill Documents and Sections\n",
    "\"\"\"    \n",
    "def laod_bill_document_and_section_count_vectorizer():\n",
    "    \n",
    "    # load count vectorize instance for doc_count_vectorizer\n",
    "    doc_count_vectorizer = pickle.load(open(\"doc_count_vectorizer.pickel\", \"rb\"))\n",
    "\n",
    "\n",
    "    # load count vectorize instance for sec_count_vectorizer\n",
    "    sec_count_vectorizer = pickle.load(open(\"sec_count_vectorizer.pickel\", \"rb\"))\n",
    "    \n",
    "    return doc_count_vectorizer, sec_count_vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorized_dataframe_from_all_documents(doc_corpus_data_df, section_corpus_data_df):\n",
    "    \n",
    "    doc_count_vectorizer, sec_count_vectorizer  = laod_bill_document_and_section_count_vectorizer()\n",
    "\n",
    "    vectorized_doc_corpus_data = []\n",
    "    for doc in range(len(doc_corpus_data_df)):\n",
    "\n",
    "        bill_doc_list = doc_corpus_data_df.iloc[doc].values\n",
    "        bill_id = bill_doc_list[0]\n",
    "        bill_document_text = bill_doc_list[1]\n",
    "\n",
    "        bill_doc_vectorized = doc_count_vectorizer.transform([bill_document_text])\n",
    "        \n",
    "\n",
    "        vectorized_doc_corpus_data.append([bill_id, {\"doc\": bill_doc_vectorized.todense().tolist()}])\n",
    "\n",
    "\n",
    "    vectorized_section_corpus_data = []\n",
    "    for section in range(len(section_corpus_data_df)):\n",
    "\n",
    "        bill_doc_list = section_corpus_data_df.iloc[section].values\n",
    "\n",
    "        bill_id = bill_doc_list[0]\n",
    "        bill_section_id = bill_doc_list[1]\n",
    "        bill_section_text = bill_doc_list[2]\n",
    "\n",
    "        bill_sec_vectorized = sec_count_vectorizer.transform([bill_section_text])\n",
    "        \n",
    "        \n",
    "        vectorized_section_corpus_data.append([bill_id, bill_section_id+1, {\"doc\": bill_sec_vectorized.todense().tolist()}])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # vectorized_doc_df\n",
    "    vectorized_doc_columns = [\"Bill ID\", 'Vectorized Document']\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    vectorized_doc_df = pd.DataFrame(vectorized_doc_corpus_data, columns = vectorized_doc_columns)\n",
    "    \n",
    "    \n",
    "\n",
    "    vectorized_doc_df['Vectorized Document'] = vectorized_doc_df['Vectorized Document'].astype('object') \n",
    "    \n",
    "    #vectorized_section_df\n",
    "    vectorized_section_columns = [\"Bill ID\", 'Section Number', 'Vectorized Section']\n",
    "    \n",
    "    \n",
    "    vectorized_section_df = pd.DataFrame(vectorized_section_corpus_data, columns = vectorized_section_columns)\n",
    "    \n",
    "    vectorized_section_df['Vectorized Section'] = vectorized_section_df['Vectorized Section'].astype('object') \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return vectorized_doc_df, vectorized_section_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save vectorized documents dataframe of Bill Documents and Section to Parquet File\n",
    "\"\"\"\n",
    "\n",
    "def save_bill_documents_and_section_corpuses_to_parquet_file(vectorized_doc_df, vectorized_section_df):\n",
    "\n",
    "    vectorized_doc_df.to_parquet('vectorized_doc_df.parquet', engine='fastparquet')\n",
    "    vectorized_section_df.to_parquet('vectorized_section_df.parquet', engine='fastparquet')\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Load vectorized documents dataframe of Bill Documents and Section From Parquet File\n",
    "\"\"\"\n",
    "def load_bill_documents_and_section_corpuses_from_parquet_file():\n",
    "\n",
    "    vectorized_doc_df = pd.read_parquet('vectorized_doc_df.parquet', engine='fastparquet')\n",
    "    vectorized_section_df = pd.read_parquet('vectorized_section_df.parquet', engine='fastparquet')\n",
    "    \n",
    "    return vectorized_doc_df, vectorized_section_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculates the similarity score for a batch of documents by multiplying the panda dataframe for that batch vs. the dataframe for the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform document into vectorized space\n",
    "def document_count_vectorized_transformation(document, doc_count_vectorizer):\n",
    "    \n",
    "    doc_vectorized = doc_count_vectorizer.transform([document])\n",
    "    return doc_vectorized\n",
    "\n",
    "def section_doc_count_vectorized_transformation(section_doc, sec_count_vectorizer):\n",
    "    \n",
    "    section_doc_vectorized = sec_count_vectorizer.transform(section_doc)\n",
    "    return section_doc_vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine pairwise similarity\n",
    "def cosine_pairwise_sim(a_vectorized, b_vectorized):\n",
    "    \n",
    "    #record time for computing similarity \n",
    "    start = time.time()\n",
    "\n",
    "    sim_score =  cosine_similarity(a_vectorized, b_vectorized)\n",
    "\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    return elapsed, sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list response\n",
    "def create_list_response(A_doc_name, B_doc_name, doc_sim_score, sec_doc_sim_score):\n",
    "    \n",
    "    #record time for creating list response\n",
    "    start = time.time()\n",
    "    \n",
    "    #create result list\n",
    "    res_list = []\n",
    "\n",
    "    #create empty list\n",
    "    temp=[]\n",
    "    temp.append(\"ORIGINAL DOCUMENT ID: \" + A_doc_name)\n",
    "    temp.append(\"MATCHED DOCUMENT ID: \" + B_doc_name)\n",
    "    temp.append(\"DOCUMENT SIMILARITY SCORE: \" + str(doc_sim_score[0][0]))\n",
    "\n",
    "    #iterate over sec_doc_sim_score list \n",
    "    for i, section_score_list in enumerate(sec_doc_sim_score):\n",
    "        \n",
    "        #add original document sentence id number\n",
    "        temp.append(\"ORIGINAL SENTENCE ID: \" + str(i+1))\n",
    "           \n",
    "        #sort similarity score of sections list\n",
    "        section_score_list = list(enumerate(section_score_list))\n",
    "        sorted_section_score_list = sorted(section_score_list, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        #iterate over section level score only \n",
    "        for j, sim_score in sorted_section_score_list:\n",
    "            temp.append({\"MATCHED DOCUMENT ID\":  B_doc_name, \"MATCHED SENTENCE ID\": j+1 , \"SENTENCE SIMILARITY SCORE\":  sim_score})\n",
    "\n",
    "    res_list.append(temp)\n",
    "    \n",
    "    \n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    \n",
    "    return elapsed, res_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bill_id = \"BILLS-117hconres11eh\"\n",
    "\n",
    "def calculate_document_similarity_from_pandas_dataframe(bill_id):\n",
    "\n",
    "    temp = vectorized_doc_df.loc[vectorized_doc_df['Bill ID'] == bill_id]\n",
    "\n",
    "    df = vectorized_doc_df\n",
    "\n",
    "    result_doc = []\n",
    "\n",
    "    #find_similar_df = df[df[\"Bill ID\"] == bill_id]\n",
    "    whole_df = df[df[\"Bill ID\"] != bill_id]\n",
    "\n",
    "    for i in range(len(whole_df)):\n",
    "\n",
    "\n",
    "\n",
    "        time_,sim_score = cosine_pairwise_sim(temp.iloc[0]['Vectorized Document']['doc'], \n",
    "                                              whole_df.iloc[i]['Vectorized Document']['doc'])\n",
    "\n",
    "\n",
    "\n",
    "        result_doc.append([bill_id , whole_df.iloc[i]['Bill ID'],sim_score[0][0]])\n",
    "\n",
    "\n",
    "    result_doc_df_columns = [\"Source Bill ID\", 'Target Bill ID', 'Doc Sim Score']\n",
    "\n",
    "\n",
    "    result_doc_df = pd.DataFrame(result_doc, columns = result_doc_df_columns)\n",
    "    return result_doc_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#bill_id = \"BILLS-117hconres11eh\"\n",
    "\n",
    "def calculate_section_similarity_from_pandas_dataframe(bill_id):\n",
    "\n",
    "    temp_1 = vectorized_section_df[vectorized_section_df[\"Bill ID\"] != bill_id]\n",
    "    temp_2 = vectorized_section_df.loc[vectorized_section_df['Bill ID'] == bill_id]\n",
    "\n",
    "    result_doc =[]\n",
    "\n",
    "    for i in range(len(temp_1)):\n",
    "        #print(temp_1.iloc[i])\n",
    "        #print(temp.iloc[i]['Bill ID'])\n",
    "        #print(temp.iloc[i]['Vectorized Section'])\n",
    "\n",
    "        for j in range(len(temp_2)):\n",
    "\n",
    "\n",
    "            time_,sim_score = cosine_pairwise_sim(temp_1.iloc[i]['Vectorized Section']['doc'], \n",
    "                                              temp_2.iloc[j]['Vectorized Section']['doc'])\n",
    "\n",
    "            result_doc.append([ temp_2.iloc[j]['Bill ID'], \n",
    "                  temp_2.iloc[j]['Section Number'],temp_1.iloc[i]['Bill ID'], temp_1.iloc[i]['Section Number'],\n",
    "\n",
    "                  sim_score[0][0]])\n",
    "\n",
    "            #print(temp_2.iloc[j]['Bill ID'], \n",
    "                  #temp_2.iloc[j]['Section Number'], temp_1.iloc[i]['Bill ID'], temp_1.iloc[i]['Section Number'], \n",
    "\n",
    "                  #sim_score)\n",
    "\n",
    "\n",
    "\n",
    "    result_sec_df_columns = [\"Source Bill ID\", \"Source Section Number\", 'Target Bill ID', 'Target Section Number', 'Section Sim Score']\n",
    "\n",
    "\n",
    "    result_sec_df = pd.DataFrame(result_doc, columns = result_sec_df_columns)\n",
    "    return result_sec_df       \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_by_matrix_multiplication_from_pandas_dataframe(bill_id):\n",
    "    \n",
    "    result_doc_df = calculate_document_similarity_from_pandas_dataframe(bill_id)\n",
    "    result_sec_df = calculate_section_similarity_from_pandas_dataframe(bill_id)\n",
    "    \n",
    "    merged_data= result_sec_df.merge(result_doc_df, on=[\"Source Bill ID\",\"Target Bill ID\"])\n",
    "   \n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filters_the_results_by_two_thresholds(result_df, threshold_for_whole_bill, threshold_for_sections):\n",
    "    \n",
    "    result_df = result_df.loc[(result_df['Doc Sim Score'] >= threshold_for_whole_bill) & (result_df['Section Sim Score'] >= threshold_for_sections)]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9168\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #threshold for whole bill  \n",
    "    threshold_for_whole_bill = 0.7\n",
    "    \n",
    "    #threshold for sections.\n",
    "    threshold_for_sections = 0.3\n",
    "\n",
    "    \n",
    "    #create document and sections data list after xml document level and section level parsing\n",
    "    \n",
    "    directory_path = PATH_117_USLM\n",
    "    doc_corpus_data, section_corpus_data = create_document_and_sections_data_list(directory_path)\n",
    "    \n",
    "    \n",
    "    #create pandas_dataframe for storing bill document and sections data with their ID and Text\n",
    "    doc_corpus_data_df, section_corpus_data_df = create_pandas_dataframe_for_bill_document_and_sections(doc_corpus_data, section_corpus_data)\n",
    "    \n",
    "    \n",
    "    #Save Vocabulary Corpus of Bill Documents and Section to Parquet File\n",
    "    save_bill_documents_and_section_corpuses_to_parquet_file(doc_corpus_data_df, section_corpus_data_df)\n",
    "    \n",
    "    #Train Count Vectorizer for Bill Documents and Sections using Pandas Dataframe\n",
    "    doc_count_vectorizer, sec_count_vectorizer = train_count_vectorizer_on_bill_document_and_sections(doc_corpus_data_df, section_corpus_data_df)\n",
    "    store_bill_document_and_section_count_vectorizer(doc_count_vectorizer, sec_count_vectorizer)\n",
    "    \n",
    "    \n",
    "    #create vectorized dataframe\n",
    "    vectorized_doc_df, vectorized_section_df = create_vectorized_dataframe_from_all_documents(doc_corpus_data_df, section_corpus_data_df)\n",
    "    \n",
    "    \n",
    "    #save vectorized dfs into parquet format\n",
    "    save_bill_documents_and_section_corpuses_to_parquet_file(vectorized_doc_df, vectorized_section_df)\n",
    "    \n",
    "    #load vectorized dfs from memory\n",
    "    vectorized_doc_df, vectorized_section_df = load_bill_documents_and_section_corpuses_from_parquet_file()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #Enter any Bill ID to find similarity using matrix multiplication of [bill1]*[ALL BILLS] to calculate all similiarities at once.\n",
    "    bill_id = \"BILLS-117hconres11eh\"\n",
    "\n",
    "    result_df = calculate_similarity_by_matrix_multiplication_from_pandas_dataframe(bill_id)\n",
    "\n",
    "    #Filters the results by two thresholds: a) threshold for whole bill and b) threshold for sections.\n",
    "    result_df  = filters_the_results_by_two_thresholds(result_df, threshold_for_whole_bill, threshold_for_sections)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Bill ID</th>\n",
       "      <th>Source Section Number</th>\n",
       "      <th>Target Bill ID</th>\n",
       "      <th>Target Section Number</th>\n",
       "      <th>Section Sim Score</th>\n",
       "      <th>Doc Sim Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>1</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>2</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>2</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>3</td>\n",
       "      <td>0.736403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>2</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>4</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>4</td>\n",
       "      <td>0.736403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>4</td>\n",
       "      <td>0.737461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>6</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>7</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>8</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>9</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>10</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>11</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>12</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>13</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>14</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>15</td>\n",
       "      <td>BILLS-117hconres11ih</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>1</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>2</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>2</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>3</td>\n",
       "      <td>0.736403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>2</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>4</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>4</td>\n",
       "      <td>0.736403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>4</td>\n",
       "      <td>0.737461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>3</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>5</td>\n",
       "      <td>0.972472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>4</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>5</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>6</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>7</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>8</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>9</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>10</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>11</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>12</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>13</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>14</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>BILLS-117hconres11eh</td>\n",
       "      <td>15</td>\n",
       "      <td>BILLS-117hconres11pcs</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source Bill ID  Source Section Number         Target Bill ID  \\\n",
       "45   BILLS-117hconres11eh                      1   BILLS-117hconres11ih   \n",
       "61   BILLS-117hconres11eh                      2   BILLS-117hconres11ih   \n",
       "63   BILLS-117hconres11eh                      4   BILLS-117hconres11ih   \n",
       "77   BILLS-117hconres11eh                      3   BILLS-117hconres11ih   \n",
       "78   BILLS-117hconres11eh                      4   BILLS-117hconres11ih   \n",
       "79   BILLS-117hconres11eh                      5   BILLS-117hconres11ih   \n",
       "91   BILLS-117hconres11eh                      2   BILLS-117hconres11ih   \n",
       "92   BILLS-117hconres11eh                      3   BILLS-117hconres11ih   \n",
       "93   BILLS-117hconres11eh                      4   BILLS-117hconres11ih   \n",
       "94   BILLS-117hconres11eh                      5   BILLS-117hconres11ih   \n",
       "107  BILLS-117hconres11eh                      3   BILLS-117hconres11ih   \n",
       "108  BILLS-117hconres11eh                      4   BILLS-117hconres11ih   \n",
       "109  BILLS-117hconres11eh                      5   BILLS-117hconres11ih   \n",
       "125  BILLS-117hconres11eh                      6   BILLS-117hconres11ih   \n",
       "141  BILLS-117hconres11eh                      7   BILLS-117hconres11ih   \n",
       "157  BILLS-117hconres11eh                      8   BILLS-117hconres11ih   \n",
       "173  BILLS-117hconres11eh                      9   BILLS-117hconres11ih   \n",
       "189  BILLS-117hconres11eh                     10   BILLS-117hconres11ih   \n",
       "205  BILLS-117hconres11eh                     11   BILLS-117hconres11ih   \n",
       "221  BILLS-117hconres11eh                     12   BILLS-117hconres11ih   \n",
       "237  BILLS-117hconres11eh                     13   BILLS-117hconres11ih   \n",
       "253  BILLS-117hconres11eh                     14   BILLS-117hconres11ih   \n",
       "269  BILLS-117hconres11eh                     15   BILLS-117hconres11ih   \n",
       "270  BILLS-117hconres11eh                      1  BILLS-117hconres11pcs   \n",
       "286  BILLS-117hconres11eh                      2  BILLS-117hconres11pcs   \n",
       "288  BILLS-117hconres11eh                      4  BILLS-117hconres11pcs   \n",
       "302  BILLS-117hconres11eh                      3  BILLS-117hconres11pcs   \n",
       "303  BILLS-117hconres11eh                      4  BILLS-117hconres11pcs   \n",
       "304  BILLS-117hconres11eh                      5  BILLS-117hconres11pcs   \n",
       "316  BILLS-117hconres11eh                      2  BILLS-117hconres11pcs   \n",
       "317  BILLS-117hconres11eh                      3  BILLS-117hconres11pcs   \n",
       "318  BILLS-117hconres11eh                      4  BILLS-117hconres11pcs   \n",
       "319  BILLS-117hconres11eh                      5  BILLS-117hconres11pcs   \n",
       "332  BILLS-117hconres11eh                      3  BILLS-117hconres11pcs   \n",
       "333  BILLS-117hconres11eh                      4  BILLS-117hconres11pcs   \n",
       "334  BILLS-117hconres11eh                      5  BILLS-117hconres11pcs   \n",
       "350  BILLS-117hconres11eh                      6  BILLS-117hconres11pcs   \n",
       "366  BILLS-117hconres11eh                      7  BILLS-117hconres11pcs   \n",
       "382  BILLS-117hconres11eh                      8  BILLS-117hconres11pcs   \n",
       "398  BILLS-117hconres11eh                      9  BILLS-117hconres11pcs   \n",
       "414  BILLS-117hconres11eh                     10  BILLS-117hconres11pcs   \n",
       "430  BILLS-117hconres11eh                     11  BILLS-117hconres11pcs   \n",
       "446  BILLS-117hconres11eh                     12  BILLS-117hconres11pcs   \n",
       "462  BILLS-117hconres11eh                     13  BILLS-117hconres11pcs   \n",
       "478  BILLS-117hconres11eh                     14  BILLS-117hconres11pcs   \n",
       "494  BILLS-117hconres11eh                     15  BILLS-117hconres11pcs   \n",
       "\n",
       "     Target Section Number  Section Sim Score  Doc Sim Score  \n",
       "45                       1           1.000000            1.0  \n",
       "61                       2           1.000000            1.0  \n",
       "63                       2           0.448719            1.0  \n",
       "77                       3           1.000000            1.0  \n",
       "78                       3           0.736403            1.0  \n",
       "79                       3           0.972472            1.0  \n",
       "91                       4           0.448719            1.0  \n",
       "92                       4           0.736403            1.0  \n",
       "93                       4           1.000000            1.0  \n",
       "94                       4           0.737461            1.0  \n",
       "107                      5           0.972472            1.0  \n",
       "108                      5           0.737461            1.0  \n",
       "109                      5           1.000000            1.0  \n",
       "125                      6           1.000000            1.0  \n",
       "141                      7           1.000000            1.0  \n",
       "157                      8           1.000000            1.0  \n",
       "173                      9           1.000000            1.0  \n",
       "189                     10           1.000000            1.0  \n",
       "205                     11           1.000000            1.0  \n",
       "221                     12           1.000000            1.0  \n",
       "237                     13           1.000000            1.0  \n",
       "253                     14           1.000000            1.0  \n",
       "269                     15           1.000000            1.0  \n",
       "270                      1           1.000000            1.0  \n",
       "286                      2           1.000000            1.0  \n",
       "288                      2           0.448719            1.0  \n",
       "302                      3           1.000000            1.0  \n",
       "303                      3           0.736403            1.0  \n",
       "304                      3           0.972472            1.0  \n",
       "316                      4           0.448719            1.0  \n",
       "317                      4           0.736403            1.0  \n",
       "318                      4           1.000000            1.0  \n",
       "319                      4           0.737461            1.0  \n",
       "332                      5           0.972472            1.0  \n",
       "333                      5           0.737461            1.0  \n",
       "334                      5           1.000000            1.0  \n",
       "350                      6           1.000000            1.0  \n",
       "366                      7           1.000000            1.0  \n",
       "382                      8           1.000000            1.0  \n",
       "398                      9           1.000000            1.0  \n",
       "414                     10           1.000000            1.0  \n",
       "430                     11           1.000000            1.0  \n",
       "446                     12           1.000000            1.0  \n",
       "462                     13           1.000000            1.0  \n",
       "478                     14           1.000000            1.0  \n",
       "494                     15           1.000000            1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
