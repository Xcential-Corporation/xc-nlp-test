{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document and Section Similarity Score Predictor using Hash Vectorizer with Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import re, string\n",
    "import os\n",
    "from os import path, listdir\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join\n",
    "from types import new_class\n",
    "from typing import List\n",
    "from lxml import etree \n",
    "from contextlib import ExitStack\n",
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer, TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Among the larger bills is samples/congress/116/BILLS-116s1790enr.xml (~ 10MB)\n",
    "\n",
    "PATH_116_USLM = 'data/samples/congress/116/uslm'\n",
    "PATH_117_USLM = 'data/samples/congress/117/uslm'\n",
    "PATH_116_USLM_TRAIN = 'data/samples/congress/116/train'\n",
    "PATH_116_TEXT = 'data/samples/congress/116/txt'\n",
    "\n",
    "BILLS_SAMPLE = [f'BILLS-116hr{number}ih.xml' for number in range(100, 300)]\n",
    "BIG_BILLS = ['BILLS-116s1790enr.xml', 'BILLS-116hjres31enr.xml']\n",
    "BIG_BILLS_PATHS = [path.join(PATH_116_USLM, bill) for bill in (BIG_BILLS + BILLS_SAMPLE)]\n",
    "\n",
    "SAMPLE_BILL_PATHS_TRAIN = [join(PATH_116_USLM_TRAIN, f) for f in listdir(PATH_116_USLM) if isfile(join(PATH_116_USLM_TRAIN, f))]\n",
    "SAMPLE_BILL_PATHS = [join(PATH_117_USLM, f) for f in listdir(PATH_117_USLM) if isfile(join(PATH_117_USLM, f))]\n",
    "\n",
    "\n",
    "NAMESPACES = {'uslm': 'http://xml.house.gov/schemas/uslm/1.0'}\n",
    "\n",
    "\n",
    "def get_filepaths(dirpath: str, reMatch = r'.xml$') -> List[str]:\n",
    "    return [join(dirpath, f) for f in listdir(dirpath) if (len(re.findall(reMatch, f)) > 0) and isfile(join(dirpath, f))]\n",
    "\n",
    "def getEnum(section) -> str:\n",
    "  enumpath = section.xpath('enum')  \n",
    "  if len(enumpath) > 0:\n",
    "    return enumpath[0].text\n",
    "  return ''\n",
    "\n",
    "def getHeader(section) -> str:\n",
    "  headerpath = section.xpath('header')  \n",
    "  if len(headerpath) > 0:\n",
    "    return headerpath[0].text\n",
    "  return ''\n",
    "\n",
    "def text_to_vect(txt: str , ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets ngrams from text\n",
    "    \"\"\"\n",
    "    # See https://stackoverflow.com/a/32128803/628748\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(txt)\n",
    "    #vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "    #    tokenizer=TreebankWordTokenizer().tokenize, lowercase=True)\n",
    "    vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "        tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    vect.fit(sentences)\n",
    "    # ngrams = vect.get_feature_names_out()\n",
    "    # print('{1}-grams: {0}'.format(ngrams, ngram_size))\n",
    "    #print(vect.vocabulary_)\n",
    "    return vect # list of text documents\n",
    "\n",
    "def xml_to_sections(xml_path: str):\n",
    "    \"\"\"\n",
    "    Parses the xml file into sections \n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    sections = billTree.xpath('//uslm:section', namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return []\n",
    "    return [{\n",
    "            'section_number': getEnum(section) ,\n",
    "            'section_header':  getHeader(section),\n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } if (section.xpath('header') and len(section.xpath('header')) > 0  and section.xpath('enum') and len(section.xpath('enum'))>0) else\n",
    "        {\n",
    "            'section_number': '',\n",
    "            'section_header': '', \n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } \n",
    "        for section in sections ]\n",
    "\n",
    "def xml_to_text(xml_path: str, level: str = 'section', separator: str = '\\n*****\\n') -> str:\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    #return etree.tostring(billTree, method=\"text\", encoding=\"unicode\")\n",
    "    # Use 'body' for level to get the whole body element\n",
    "    sections = billTree.xpath('//uslm:'+level, namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return '' \n",
    "    return separator.join([etree.tostring(section, method=\"text\", encoding=\"unicode\") for section in sections])\n",
    "\n",
    "def xml_to_vect(xml_paths: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    total_str = '\\n'.join([xml_to_text(xml_path) for xml_path in xml_paths])\n",
    "    return text_to_vect(total_str, ngram_size=ngram_size)\n",
    "\n",
    "    # to get the vocab dict: vect.vocabulary_\n",
    "\n",
    "def combine_vocabs(vocabs: List[CountVectorizer]):\n",
    "    \"\"\"\n",
    "    Combines one or more vocabs into one\n",
    "    \"\"\"\n",
    "    vocab_keys = list(set([list(v.vocabulary_.keys()) for v in vocabs]))\n",
    "    vocab = {vocab_key: str(i) for i, vocab_key in enumerate(vocab_keys)}\n",
    "    return vocab\n",
    "\n",
    "def get_combined_vocabs(xml_paths: List[str] = SAMPLE_BILL_PATHS, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets the combined vocabulary of all the xml files\n",
    "    \"\"\"\n",
    "    return xml_to_vect(xml_paths, ngram_size=ngram_size)\n",
    "\n",
    "def getSampleText(level = 'body'):\n",
    "    return xml_to_text(BIG_BILLS_PATHS[0])\n",
    "\n",
    "def transform_text(text: str, vocab: dict, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Transforms text into a vector using the vocab\n",
    "    \"\"\"\n",
    "    return CountVectorizer(vocabulary=vocab).fit_transform([text])\n",
    "\n",
    "def train_count_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a count vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X \n",
    "\n",
    "def train_hashing_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a hashing vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = HashingVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X\n",
    "\n",
    "def test_hashing_vectorizer(vectorizer: HashingVectorizer, test_data: List[str]):\n",
    "    return vectorizer.transform(test_data)\n",
    "\n",
    "def xml_samples_to_text(dirpath: str, level: str = 'section', separator: str = '\\n*****\\n'):\n",
    "    \"\"\"\n",
    "    Converts xml files in a directory to txt files\n",
    "    \"\"\"\n",
    "    xfiles = get_filepaths(dirpath)\n",
    "    for xfile in xfiles:\n",
    "        with open(xfile.replace('.xml', f'-{level}s.txt'), 'w') as f:\n",
    "            f.write(xml_to_text(xfile, level=level, separator=separator))\n",
    "\n",
    "# TODO: Add a function to parse the bill (text) into paragraphs \n",
    "\n",
    "# TODO: create a streaming hash vectorizer. See \n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text \n",
    "def text_cleaning(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Transform Document into vectorized space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform document into vectorized space\n",
    "def document_hash_vectorized_transformation(document, doc_hash_vectorizer):\n",
    "    \n",
    "    doc_vectorized = doc_hash_vectorizer.transform([document])\n",
    "    return doc_vectorized\n",
    "\n",
    "def section_doc_hash_vectorized_transformation(section_doc, sec_hash_vectorizer):\n",
    "    \n",
    "    section_doc_vectorized = sec_hash_vectorizer.transform(section_doc)\n",
    "    return section_doc_vectorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sections found\n",
      "No sections found\n",
      "9166\n",
      "55007\n",
      "Time took in ETL with 9166 xml data files is 155.78053665161133\n"
     ]
    }
   ],
   "source": [
    "#xml document and section level parsing\n",
    "\n",
    "#record training time for both vectorizer\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "doc_corpus_data=[]\n",
    "section_corpus_data = []\n",
    "\n",
    "#get all xml files from data directory for parsing\n",
    "bill_files = [f for f in os.listdir(PATH_117_USLM) if f.endswith('.xml')]\n",
    "\n",
    "#iterate over all bill files\n",
    "for i in range(0, len(bill_files)):\n",
    "    \n",
    "    #indexing bill document file\n",
    "    bill_doc_file = bill_files[i]\n",
    "    \n",
    "    #parse xml into sections\n",
    "    secs = xml_to_sections(os.path.join(PATH_117_USLM, bill_doc_file))\n",
    "    \n",
    "    #check  of sections should be 1 or more than 1\n",
    "    if(len(secs)>0):  \n",
    "        \n",
    "        #intialize string variable for document content\n",
    "        doc_content = \"\"\n",
    "        \n",
    "        #iterate over all parse sections text of bill doc file\n",
    "        for s_number, section in enumerate(secs):  \n",
    "            \n",
    "            #text cleaning applied on each section text\n",
    "            sec_text = text_cleaning(section['section_text'])\n",
    "            \n",
    "            #concatenate section text to doc content \n",
    "            doc_content = doc_content + sec_text + \" \"\n",
    "            \n",
    "             #for now sentence id is sentence number in document\n",
    "            section_corpus_data.append([Path(bill_doc_file).stem[:], s_number, sec_text ])\n",
    "\n",
    "        doc_corpus_data.append([Path(bill_doc_file).stem[:], doc_content])\n",
    "    \n",
    "\n",
    "#get only whole document content from doc_corpus_data list\n",
    "only_doc_data = [row[1] for row in doc_corpus_data]\n",
    "\n",
    "#get only section content from section_corpus_data list\n",
    "only_section_data = [row[2] for row in section_corpus_data]\n",
    "\n",
    "#store pre-processed document corpus and section level corpus\n",
    "pickle.dump(doc_corpus_data, open(\"hv_doc_corpus_data.pickel\", \"wb\"))\n",
    "pickle.dump(section_corpus_data, open(\"hv_section_corpus_data.pickel\", \"wb\"))\n",
    "\n",
    "#get length of only_doc_data list\n",
    "print(len(only_doc_data))\n",
    "\n",
    "#get length of only_section_data list\n",
    "print(len(only_section_data))\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print('Time took in ETL with {} xml data files is {}'.format(len(only_doc_data), elapsed))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khizer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time took in training of both vectorizer(s)  135.96929478645325\n"
     ]
    }
   ],
   "source": [
    "#record training time for both vectorizer\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Vectorizer to convert a collection of raw documents to a matrix \n",
    "doc_hash_vectorizer = HashingVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "#Fit hash vectorize instance on document level corpus\n",
    "hv_doc_matrix = doc_hash_vectorizer.fit_transform(only_doc_data)\n",
    "\n",
    "# Vectorizer to convert a collection of sections to a matrix \n",
    "sec_hash_vectorizer = HashingVectorizer(ngram_range=(4,4), tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "#Fit hash vectorize instance on section level corpus\n",
    "hv_section_matrix = sec_hash_vectorizer.fit_transform(only_section_data)\n",
    "\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(\"Time took in training of both vectorizer(s) \", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hash vectorize instance for only_doc_data\n",
    "pickle.dump(doc_hash_vectorizer, open(\"doc_hash_vectorizer.pickel\", \"wb\"))\n",
    "# load hash vectorize instance for only_doc_data\n",
    "doc_hash_vectorizer = pickle.load(open(\"doc_hash_vectorizer.pickel\", \"rb\"))\n",
    "\n",
    "#save hash vectorize instance for only_section_data\n",
    "pickle.dump(sec_hash_vectorizer, open(\"sec_hash_vectorizer.pickel\", \"wb\"))\n",
    "# load hash vectorize instance for only_section_data\n",
    "sec_hash_vectorizer = pickle.load(open(\"sec_hash_vectorizer.pickel\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
