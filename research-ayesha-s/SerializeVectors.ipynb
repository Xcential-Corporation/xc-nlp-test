{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a3da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import re, string\n",
    "import os\n",
    "from os import path, listdir\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join\n",
    "from types import new_class\n",
    "from typing import List\n",
    "from lxml import etree \n",
    "from contextlib import ExitStack\n",
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer, TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Among the larger bills is samples/congress/116/BILLS-116s1790enr.xml (~ 10MB)\n",
    "\n",
    "PATH_116_USLM = 'data/samples/congress/116/uslm'\n",
    "PATH_117_USLM = 'data/samples/congress/117/uslm'\n",
    "PATH_116_USLM_TRAIN = 'data/samples/congress/116/train'\n",
    "PATH_116_TEXT = 'data/samples/congress/116/txt'\n",
    "\n",
    "BILLS_SAMPLE = [f'BILLS-116hr{number}ih.xml' for number in range(100, 300)]\n",
    "BIG_BILLS = ['BILLS-116s1790enr.xml', 'BILLS-116hjres31enr.xml']\n",
    "BIG_BILLS_PATHS = [path.join(PATH_116_USLM, bill) for bill in (BIG_BILLS + BILLS_SAMPLE)]\n",
    "\n",
    "SAMPLE_BILL_PATHS_TRAIN = [join(PATH_116_USLM_TRAIN, f) for f in listdir(PATH_116_USLM) if isfile(join(PATH_116_USLM_TRAIN, f))]\n",
    "SAMPLE_BILL_PATHS = [join(PATH_117_USLM, f) for f in listdir(PATH_117_USLM) if isfile(join(PATH_117_USLM, f))]\n",
    "\n",
    "\n",
    "NAMESPACES = {'uslm': 'http://xml.house.gov/schemas/uslm/1.0'}\n",
    "\n",
    "\n",
    "def get_filepaths(dirpath: str, reMatch = r'.xml$') -> List[str]:\n",
    "    return [join(dirpath, f) for f in listdir(dirpath) if (len(re.findall(reMatch, f)) > 0) and isfile(join(dirpath, f))]\n",
    "\n",
    "def getEnum(section) -> str:\n",
    "  enumpath = section.xpath('enum')  \n",
    "  if len(enumpath) > 0:\n",
    "    return enumpath[0].text\n",
    "  return ''\n",
    "\n",
    "def getHeader(section) -> str:\n",
    "  headerpath = section.xpath('header')  \n",
    "  if len(headerpath) > 0:\n",
    "    return headerpath[0].text\n",
    "  return ''\n",
    "\n",
    "def text_to_vect(txt: str , ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets ngrams from text\n",
    "    \"\"\"\n",
    "    # See https://stackoverflow.com/a/32128803/628748\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = tokenizer.tokenize(txt)\n",
    "    #vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "    #    tokenizer=TreebankWordTokenizer().tokenize, lowercase=True)\n",
    "    vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size),\n",
    "        tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    vect.fit(sentences)\n",
    "    # ngrams = vect.get_feature_names_out()\n",
    "    # print('{1}-grams: {0}'.format(ngrams, ngram_size))\n",
    "    #print(vect.vocabulary_)\n",
    "    return vect # list of text documents\n",
    "\n",
    "def xml_to_sections(xml_path: str):\n",
    "    \"\"\"\n",
    "    Parses the xml file into sections \n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    sections = billTree.xpath('//uslm:section', namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return []\n",
    "    return [{\n",
    "            'section_number': getEnum(section) ,\n",
    "            'section_header':  getHeader(section),\n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } if (section.xpath('header') and len(section.xpath('header')) > 0  and section.xpath('enum') and len(section.xpath('enum'))>0) else\n",
    "        {\n",
    "            'section_number': '',\n",
    "            'section_header': '', \n",
    "            'section_text': etree.tostring(section, method=\"text\", encoding=\"unicode\"),\n",
    "            'section_xml': etree.tostring(section, method=\"xml\", encoding=\"unicode\")\n",
    "        } \n",
    "        for section in sections ]\n",
    "\n",
    "def xml_to_text(xml_path: str, level: str = 'section', separator: str = '\\n*****\\n') -> str:\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        billTree = etree.parse(xml_path)\n",
    "    except:\n",
    "        raise Exception('Could not parse bill')\n",
    "    #return etree.tostring(billTree, method=\"text\", encoding=\"unicode\")\n",
    "    # Use 'body' for level to get the whole body element\n",
    "    sections = billTree.xpath('//uslm:'+level, namespaces=NAMESPACES)\n",
    "    if len(sections) == 0:\n",
    "        print('No sections found')\n",
    "        return '' \n",
    "    return separator.join([etree.tostring(section, method=\"text\", encoding=\"unicode\") for section in sections])\n",
    "\n",
    "def xml_to_vect(xml_paths: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Parses the xml file and returns the text of the body element, if any\n",
    "    \"\"\"\n",
    "    total_str = '\\n'.join([xml_to_text(xml_path) for xml_path in xml_paths])\n",
    "    return text_to_vect(total_str, ngram_size=ngram_size)\n",
    "\n",
    "    # to get the vocab dict: vect.vocabulary_\n",
    "\n",
    "def combine_vocabs(vocabs: List[CountVectorizer]):\n",
    "    \"\"\"\n",
    "    Combines one or more vocabs into one\n",
    "    \"\"\"\n",
    "    vocab_keys = list(set([list(v.vocabulary_.keys()) for v in vocabs]))\n",
    "    vocab = {vocab_key: str(i) for i, vocab_key in enumerate(vocab_keys)}\n",
    "    return vocab\n",
    "\n",
    "def get_combined_vocabs(xml_paths: List[str] = SAMPLE_BILL_PATHS, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Gets the combined vocabulary of all the xml files\n",
    "    \"\"\"\n",
    "    return xml_to_vect(xml_paths, ngram_size=ngram_size)\n",
    "\n",
    "def getSampleText(level = 'body'):\n",
    "    return xml_to_text(BIG_BILLS_PATHS[0])\n",
    "\n",
    "def transform_text(text: str, vocab: dict, ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Transforms text into a vector using the vocab\n",
    "    \"\"\"\n",
    "    return CountVectorizer(vocabulary=vocab).fit_transform([text])\n",
    "\n",
    "def train_count_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a count vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X \n",
    "\n",
    "def train_hashing_vectorizer(train_data: List[str], ngram_size: int = 4):\n",
    "    \"\"\"\n",
    "    Trains a hashing vectorizer on the training data\n",
    "    \"\"\"\n",
    "    vectorizer = HashingVectorizer(ngram_range=(ngram_size,ngram_size), preprocessor=xml_to_text, tokenizer=RegexpTokenizer(r\"\\w+\").tokenize, lowercase=True)\n",
    "    with ExitStack() as stack:\n",
    "        files = [\n",
    "            stack.enter_context(open(filename))\n",
    "            for filename in train_data \n",
    "        ]\n",
    "        X = vectorizer.fit_transform(files)\n",
    "    return vectorizer, X\n",
    "\n",
    "def test_hashing_vectorizer(vectorizer: HashingVectorizer, test_data: List[str]):\n",
    "    return vectorizer.transform(test_data)\n",
    "\n",
    "def xml_samples_to_text(dirpath: str, level: str = 'section', separator: str = '\\n*****\\n'):\n",
    "    \"\"\"\n",
    "    Converts xml files in a directory to txt files\n",
    "    \"\"\"\n",
    "    xfiles = get_filepaths(dirpath)\n",
    "    for xfile in xfiles:\n",
    "        with open(xfile.replace('.xml', f'-{level}s.txt'), 'w') as f:\n",
    "            f.write(xml_to_text(xfile, level=level, separator=separator))\n",
    "\n",
    "# TODO: Add a function to parse the bill (text) into paragraphs \n",
    "\n",
    "# TODO: create a streaming hash vectorizer. See \n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59fb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text \n",
    "def text_cleaning(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fc4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vectorize instance for only_section_data\n",
    "def get_vectorization_model():\n",
    "\n",
    "    # load tfidf vectorize instance for only_section_data\n",
    "    sec_tfidf_vectorizer = pickle.load(open(\"sec_tfidf_vectorizer.pickel\", \"rb\"))\n",
    "    \n",
    "    return sec_tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83899cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#section vectorized transformation\n",
    "def section_doc_tfidf_vectorized_transformation(section_text):\n",
    "    \n",
    "    #get vectorize instance for only_section_data\n",
    "    sec_tfidf_vectorizer = get_vectorization_model()\n",
    "    \n",
    "    section_doc_vectorized = sec_tfidf_vectorizer.transform(section_text)\n",
    "    return section_doc_vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32201162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize vectors from xml path\n",
    "def serialize_calculated_vectors(file_path):\n",
    "    \n",
    "    #create empty t_sections_data list\n",
    "    t_sections_data=[] \n",
    "    \n",
    "    #get sections fron xml file\n",
    "    t_secs = xml_to_sections(file_path)\n",
    "    \n",
    "    #check if length of section is more than 0  \n",
    "    if(len(t_secs)>0): \n",
    "        \n",
    "            #intialize string variable for document content\n",
    "            t_doc_content = \"\"\n",
    "\n",
    "            #iterate over all parse sections text of bill doc file\n",
    "            for s_number, section in enumerate(t_secs):  \n",
    "\n",
    "                #text cleaning applied on each section text\n",
    "                sec_text = text_cleaning(section['section_text'])\n",
    "\n",
    "                \n",
    "                #append to t_sections_data list\n",
    "                t_sections_data.append([s_number,sec_text])\n",
    "\n",
    "            #print(len(t_sections_data))\n",
    "                \n",
    "            #get only section content from t_sections_data list\n",
    "            only_section_data = [row[1] for row in t_sections_data]\n",
    "\n",
    "            \n",
    "            sections_doc_vectorized = section_doc_tfidf_vectorized_transformation(only_section_data)\n",
    "            \n",
    "            #Serialize this vectorization into a file\n",
    "            pickle.dump(sections_doc_vectorized, open(\"data/vectorized_data/{}.pickel\".format(Path(file_path).stem[:]), \"wb\"))\n",
    "     \n",
    "            \n",
    "            return sections_doc_vectorized\n",
    "                \n",
    "                \n",
    "           \n",
    "               \n",
    "               \n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d59fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialize calculated vectors:    (0, 4029027)\t0.16597408450798212\n",
      "  (0, 3756963)\t0.3558921936697121\n",
      "  (0, 3416300)\t0.1659503969537569\n",
      "  (0, 3327902)\t0.16600568417496245\n",
      "  (0, 3174530)\t0.35896180811991907\n",
      "  (0, 2205103)\t0.15809879156334888\n",
      "  (0, 867694)\t0.403234511847798\n",
      "  (0, 674554)\t0.15828325813706703\n",
      "  (0, 572324)\t0.5436673507268913\n",
      "  (0, 251982)\t0.35698822421119975\n",
      "  (0, 128248)\t0.16180444654952353\n",
      "  (1, 4442981)\t0.023964255086630852\n",
      "  (1, 4427207)\t0.02560507079152366\n",
      "  (1, 4426846)\t0.02560507079152366\n",
      "  (1, 4422121)\t0.023964255086630852\n",
      "  (1, 4421297)\t0.02560507079152366\n",
      "  (1, 4421263)\t0.02560507079152366\n",
      "  (1, 4418841)\t0.023964255086630852\n",
      "  (1, 4418840)\t0.023964255086630852\n",
      "  (1, 4418839)\t0.023964255086630852\n",
      "  (1, 4417677)\t0.02560507079152366\n",
      "  (1, 4416887)\t0.023964255086630852\n",
      "  (1, 4416862)\t0.02439584607065009\n",
      "  (1, 4416860)\t0.023964255086630852\n",
      "  (1, 4416857)\t0.02560507079152366\n",
      "  :\t:\n",
      "  (236, 678934)\t0.08107792117652475\n",
      "  (236, 676854)\t0.045382195052773674\n",
      "  (236, 636314)\t0.08107792117652475\n",
      "  (236, 636222)\t0.1621558423530495\n",
      "  (236, 635975)\t0.08107792117652475\n",
      "  (236, 630658)\t0.04589019680570635\n",
      "  (236, 629263)\t0.04102155436531725\n",
      "  (236, 623231)\t0.14204637991003585\n",
      "  (236, 523645)\t0.05030520348875684\n",
      "  (236, 523589)\t0.03988316094317072\n",
      "  (236, 479813)\t0.03934894198218062\n",
      "  (236, 412568)\t0.20940292254883786\n",
      "  (236, 386698)\t0.06649200375873603\n",
      "  (236, 293117)\t0.08107792117652475\n",
      "  (236, 260978)\t0.041338492061196726\n",
      "  (236, 251381)\t0.07892154876981466\n",
      "  (236, 226105)\t0.08107792117652475\n",
      "  (236, 200965)\t0.08107792117652475\n",
      "  (236, 194800)\t0.018863549576778677\n",
      "  (236, 154667)\t0.039086237991327176\n",
      "  (236, 145717)\t0.17338722223022002\n",
      "  (236, 131047)\t0.070086729521326\n",
      "  (236, 130841)\t0.10119837871238362\n",
      "  (236, 130422)\t0.1022633013871461\n",
      "  (236, 130192)\t0.040304970042440914\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    #serialize calculated vectors of any file \n",
    "    file_path = os.path.join(PATH_117_USLM, \"BILLS-117hr1319enr\"+\".xml\")\n",
    "    print(\"serialize calculated vectors: \", serialize_calculated_vectors(file_path))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65405a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
