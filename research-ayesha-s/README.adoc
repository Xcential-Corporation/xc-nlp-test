Count Vectorization:
I have trained my model for 9166 xml data files in 117.96 seconds. The saved models took 366 mb space on my disk.

Computing the similarity score for documents takes around 0.004 to 0.08 seconds duration based on the file size.

Similarity score for the similar files with different format is more than 94% (for document and section wise as well) for all most all similar files. If in some cases it is less, then it must be the difference in the data content of the files when checked.

TFIDF Vectorization :
I have trained my model for 9166 xml data files in 163. 60 seconds. The saved models took 510 mb space on my disk.

Computing the similarity score for documents takes around 0.004 to 0.08 seconds duration based on the file size.

Similarity score for the similar files with different format is more than 90% (for document and section wise as well) for all most all similar files. If in some cases it is less, then it must be the difference in the data content of the files when checked.

Hashing Vectorization:
I have trained my model for 9166 xml data files in 67.33 seconds. The saved models took only 2kb space on my disk. That is the biggest advantage. Also, we can bind this model to the api easily.

Computing the similarity score for documents takes around 0.004 to 0.08 seconds duration based on the file size.

Similarity score for the similar files with different format is more than 95% (for document and section wise as well) for all most all similar files. If in some cases it is less, then it must be the difference in the data content of the files when checked.

Time for computing the similarity is almost same for all 3 approaches due to the same time complexity of the similarity formula.